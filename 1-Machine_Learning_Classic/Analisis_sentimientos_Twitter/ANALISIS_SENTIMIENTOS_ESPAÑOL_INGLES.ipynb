{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2_ANALISIS_SENTIMIENTOS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g3w7xhrG0_q5",
        "54aCGRWaPPq6",
        "dtzJHYMJcQz3",
        "5E84yXpedZQ3",
        "5Rc9RsrNNbxI",
        "HZ4mzeX-NfBu",
        "IhCauzL5jkpN",
        "LrG7haSNtb6T",
        "xvec1PiF5Ipz",
        "j6wQAhv0V4RU",
        "zm6bJ8rDnJ8d",
        "YYuKjPlHnvdV",
        "h6a_3I3ZoTc6",
        "c0t1IZY6EzcB",
        "eqoOWZwOsD6w",
        "W712Ry66vFjl",
        "UwBHQQFsqR7i",
        "WlERt98W5qLZ",
        "IrJ7vHEWNAbd",
        "OTB4zzjoL1Rs",
        "HCCRddegNIDA",
        "p3qBGgvpRgZL",
        "FCS2HmN4V_hi",
        "-aWOdOb167P7",
        "OIJQSCntSPOl",
        "IOE6fCqXcUXn",
        "-3kl_mdMN4ec",
        "4fcBk57OXCz3",
        "MiZauNoCbrxG"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtdtWcd3avP8"
      },
      "source": [
        "#Aclaraciones de librerias\n",
        "\n",
        "*   Numpy 1.12.1\n",
        "*   scikit-learn 0.18.1\n",
        "*   Matplotlib 2.0.2\n",
        "*   Pandas 0.20.1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8xELpxScApc"
      },
      "source": [
        "!pip list\n",
        "#pip uninstall scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w0V1Cd_SSDw"
      },
      "source": [
        "!pip install scikit-learn==0.18.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP2jLRomNq8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9138d8d5-d913-4455-a966-4d9ce32d1a81"
      },
      "source": [
        "# instalar librería para visualizar el progreso de ejecución una tarea en background\n",
        "!pip install pyprind"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyprind\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/30/e76fb0c45da8aef49ea8d2a90d4e7a6877b45894c25f12fb961f009a891e/PyPrind-2.11.2-py3-none-any.whl\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4wVbh580lRc"
      },
      "source": [
        "#Habilitando google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb-6fMMG0n-l",
        "outputId": "580e9fe8-908c-427f-f3a2-dd1ac7dcce22"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_nOfJ_1K0si"
      },
      "source": [
        "#**ANALISIS DE SENTIMIENTOS EN INGLES USANDO ACLIMDB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "968LBC6YoA4A"
      },
      "source": [
        "##P0-Librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YPjnDkRKq2j"
      },
      "source": [
        "import os            #  trabajar sobre el sistema operativo\n",
        "import sys           #  manipular archivos (cortar, copiar, borrar, crear)\n",
        "import tarfile       #  Manipular archivos comprimidos (comprimir, descomprimir)\n",
        "import time          #  calcular tiempo (en este caso tiempo de descarga de archivo)\n",
        "import re            #  operaciones regulares con cadenas de texto\n",
        "import pyprind\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.pipeline import Pipeline                         # permite implementar métodos de ajuste y transformación\n",
        "from sklearn.linear_model import LogisticRegression           # modelo de regresión logistica\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer   #conversor de texto a vector\n",
        "from sklearn.model_selection import GridSearchCV              #búsqueda de cuadrícula con validación cruzada (para usar con regresión logistica)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3w7xhrG0_q5"
      },
      "source": [
        "##P1- Obteniendo corpus de aclimdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkRxGqry1O-d"
      },
      "source": [
        "basepath = 'aclImdb'\n",
        "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "target = 'aclImdb_v1.tar.gz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABUOzdraoEOT"
      },
      "source": [
        "###función para ver avance de proceso en *background*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EoFo2aPMMO5"
      },
      "source": [
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = progress_size / (1024.**2 * duration)\n",
        "    percent = count * block_size * 100. / total_size\n",
        "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d segundos transcurrido\" %\n",
        "                    (percent, progress_size / (1024.**2), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "if not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n",
        "    \n",
        "    if (sys.version_info < (3, 0)):\n",
        "        import urllib\n",
        "        urllib.urlretrieve(source, target, reporthook)\n",
        "    \n",
        "    else:\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(source, target, reporthook)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMNt5_wWLF1Z"
      },
      "source": [
        "### Extraemos todos los archivos del corpus "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiRUWiaANHaZ"
      },
      "source": [
        "if not os.path.isdir('aclImdb'):\n",
        "\n",
        "    with tarfile.open(target, 'r:gz') as tar:\n",
        "        tar.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokQ64p5nvic"
      },
      "source": [
        "###Funcion para convertir archivos txt en un dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG-anmqPOlnW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "25e7929f-8f3d-466f-ee63-ca8b229fccb5"
      },
      "source": [
        "labels = {'pos': 1, 'neg': 0}\n",
        "pbar = pyprind.ProgBar(50000)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for s in ('test', 'train'):\n",
        "    for l in ('pos', 'neg'):\n",
        "        path = os.path.join(basepath, s, l)\n",
        "        for file in os.listdir(path):\n",
        "            with open(os.path.join(path, file), \n",
        "                      'r', encoding='utf-8') as infile:\n",
        "                txt = infile.read()\n",
        "            df = df.append([[txt, labels[l]]], \n",
        "                           ignore_index=True)\n",
        "            pbar.update()\n",
        "\n",
        "df.columns = ['review', 'sentiment']\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:01:21\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ9lDR0npHou"
      },
      "source": [
        "###Guardando Dataframe en CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO_r_-BdPx8N"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ldGm4X5MfTj"
      },
      "source": [
        "##P2-Preprocesamiento del corpus "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68XefC8xQMR1"
      },
      "source": [
        "###Cargar el CSV de criticas de cine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3aqFb73QxPH"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/luisFernandoCastellanosG/Machine_learning/master/An%C3%A1lisis%20de%20sentimientos%20en%20Twitter/Ingles/Dataset/PLN_movie_data_preprocesor.csv', encoding='utf-8')\n",
        "vector= CountVectorizer(stop_words=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYxp9he-509u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5cb298-0df0-4457-fc02-46475c9c2b8d"
      },
      "source": [
        "df.head(5)\n",
        "df.sentiment.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUMScms2TODd"
      },
      "source": [
        "###Vectorizando texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UuLmL2jTPzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063ee853-b128-4fb4-9bb3-cc63731ecfb3"
      },
      "source": [
        "new_text = ['probando un texto para pruebas de texto']\n",
        "vector.fit(new_text)\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'probando': 2, 'un': 5, 'texto': 4, 'para': 1, 'pruebas': 3, 'de': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdXt41hUU08l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bce4f17-f093-4698-b9d1-8714f6a255b5"
      },
      "source": [
        "count = CountVectorizer()\n",
        "docs = np.array([\n",
        "        'The sun is shining',\n",
        "        'The weather is sweet',\n",
        "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag = count.fit_transform(docs)\n",
        "\n",
        "print(count.vocabulary_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Aq4cLPVbQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4809060c-a612-4c18-9725-98ef69b58ea5"
      },
      "source": [
        "print(bag.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 1]\n",
            " [2 3 2 1 1 1 2 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z50v53mtMwj-"
      },
      "source": [
        "###Extracción de características "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54aCGRWaPPq6"
      },
      "source": [
        "####TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDhMXl6yPY11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b013bdb-89b7-43e2-c6fc-572eb7521cba"
      },
      "source": [
        "tfidf = TfidfTransformer(use_idf=True, \n",
        "                         norm='l2', \n",
        "                         smooth_idf=True)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
            " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
            " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvquv5h2REK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d0d494-354a-40d3-a9ae-c1425180eaf5"
      },
      "source": [
        "df.head(7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>four daughters a sentimental story of a solid...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this review contains a partial spoiler shallow...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a group of us watched this film are were reall...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>if you were a director that was looking to cas...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ok i had higher hopes for this carnosaur movie...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>if the very thought of arthur askey twists you...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>i am right now in front of the tv watching cas...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0   four daughters a sentimental story of a solid...          1\n",
              "1  this review contains a partial spoiler shallow...          0\n",
              "2  a group of us watched this film are were reall...          0\n",
              "3  if you were a director that was looking to cas...          1\n",
              "4  ok i had higher hopes for this carnosaur movie...          0\n",
              "5  if the very thought of arthur askey twists you...          1\n",
              "6  i am right now in front of the tv watching cas...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BgLZO4TRb3z"
      },
      "source": [
        "####funcion para limpiar textos de emoticones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS4XmdrnRgpN"
      },
      "source": [
        "# creamos una funcion llamada preprocessor\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETxFZSMwUc5G"
      },
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBCYt1WiVZO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1ba898-583e-4b49-ac68-f91e860b08de"
      },
      "source": [
        "import nltk\n",
        "#nltk.download(“all\")\n",
        "#nltk.download(\"popular\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsfmDqa3WQoH"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQcWrE3tWCMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf2c770-6f75-4c5f-dc68-24dd870e69af"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot is very important')\n",
        "if w not in stop]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'run', 'lot', 'veri', 'import']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4WHqhJNXSd2"
      },
      "source": [
        "##P3-Entrenando modelo usando REGRESIÓN LOGISTICA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VB1z3XtXwzG"
      },
      "source": [
        "# separamos los datos de entrenamiento y de pruebas\n",
        "X_train = df.loc[:25000, 'review'].values\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = df.loc[25000:, 'review'].values\n",
        "y_test = df.loc[25000:, 'sentiment'].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTsWnpW9YCmt"
      },
      "source": [
        "#propiedades de la conversor de texto a vectores\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None)\n",
        "def tokenizer(text):\n",
        "    return text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDbMeTYeYh_0"
      },
      "source": [
        "#creación de dos diccionarios (1° para cálculos de TF-IDF, 2° para entrenar modelo con regresión logistica\n",
        "# con estos parametros dura aproximadamente 4 horas entrenando\n",
        "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              {'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'vect__use_idf':[False],\n",
        "               'vect__norm':[None],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]}, ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znMfX3ePY2W0"
      },
      "source": [
        "##Parametros light para mejorar \n",
        "param_grid = [{'vect_stop_words': [english_stopwords, None],\n",
        "               'vect_tokenizer': [tokenizer],},\n",
        "              ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAfn0TRLY_6B"
      },
      "source": [
        "#mezclamos regresión lineal y vectores de textos en un solo proceso\n",
        "lr_tfidf = Pipeline([('vect', tfidf),\n",
        "                     ('clf', LogisticRegression(random_state=0))])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75pQFTFvZUhC"
      },
      "source": [
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uGVwXBsZgHK"
      },
      "source": [
        "gs_lr_tfidf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDa5v_5B4jVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912f1335-a10f-488b-9668-8b3735ccc49d"
      },
      "source": [
        "#print('Mejores set de parametros: %s ' % gs_lr_tfidf.best_params_)\n",
        "#print('CV exactitud: %.3f' % gs_lr_tfidf.best_score_)\n",
        "clf = gs_lr_tfidf.best_estimator_\n",
        "print('Test exactitud: %.3f' % clf.score(X_test, y_test))\n",
        "#clf=clf.partial_fit(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test exactitud: 0.899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9-Uf7Zo3WD0"
      },
      "source": [
        "##P4. serialización del modelo de IA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeLBFGqs3a9n"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "#creo una carpeta en mi google drive para guardar los archivos serializados\n",
        "dest = os.path.join('/content/twitterclassifier', 'pkl_objects')\n",
        "if not os.path.exists(dest):\n",
        "    os.makedirs(dest)\n",
        "#convertimos el clasificador y el stopword en archivo/objectos pkl\n",
        "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
        "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)\n",
        "#Es importante recordar que deben verificar que los dos archivos esten en su drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBBEOt120SNB"
      },
      "source": [
        "##P5-Deserializamos los estimadores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm7lI-vm3UzC"
      },
      "source": [
        "###P5.1 crear archivo independiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFvLhgVG2i2j"
      },
      "source": [
        "**ESte es el código fuente del archivo vectorizer.py**\n",
        "\n",
        "---\n",
        "El archivo debe estar cargado en la capeta **/content/twitterclassifier/**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5hX94852w2Q"
      },
      "source": [
        "##copie este codigo y guardelo en un archivo externo llamado vectorizer.py \n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import re    #operaciones con funciones regulares con cadenas\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "cur_dir = os.path.dirname(__file__)\n",
        "stop = pickle.load(open(\n",
        "                os.path.join(cur_dir, \n",
        "                'pkl_objects', \n",
        "                'stopwords.pkl'), 'rb'))\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
        "                           text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
        "                   + ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore',\n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None,\n",
        "                         tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPgvE5q13aps"
      },
      "source": [
        "###P5.2 desserializar los archivos exportados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeQs8xpn0adc"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "import os\n",
        "\n",
        "#cambiamos el path default de python a la carpeta /content/twitterclassifier\n",
        "os.chdir('/content/twitterclassifier')\n",
        "\n",
        "#debe garantizar que el archivo vectorizer.py este en el servidor \n",
        "from vectorizer import vect\n",
        "#cargamos el clasificador \n",
        "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH1JkJ-l3kvo"
      },
      "source": [
        "###P5.3 probar clasificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL-6KHQs3olG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdaf35ad-8257-4839-8af0-0483f1eab593"
      },
      "source": [
        "import numpy as np\n",
        "#Negative->0 | positive ->1 \n",
        "label = {0:'Negative', 1:'Positive'}\n",
        "\n",
        "example1 = ['THIS MOVIE IS VERY BAD']\n",
        "example2 = ['I LOVE THIS MOVIE, for me is the best movie in the world']\n",
        "#convertimos el texto en un vector de palabras y extraemos sus caracteristicas\n",
        "#textConvert = vect.transform(example1) \n",
        "print(example1)\n",
        "print('*Predicción: %s *Probabilidad: %.2f%%'%(label[clf.predict(example1)[0]], np.max(clf.predict_proba(example1))*100))\n",
        "print('\\n'+str(example2))\n",
        "print('*Predicción: %s *Probabilidad: %.2f%%'%(label[clf.predict(example2)[0]], np.max(clf.predict_proba(example2))*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['THIS MOVIE IS VERY BAD']\n",
            "*Predicción: Negative *Probabilidad: 57.31%\n",
            "\n",
            "['I LOVE THIS MOVIE, for me is the best movie in the world']\n",
            "*Predicción: Positive *Probabilidad: 99.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDXCWuClqs4S"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**ANALISIS DE SENTIMIENTOS EN ESPAÑOL USANDO TWITTER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Ztco6-b09l"
      },
      "source": [
        "##Obtener Dataset de twitter para analizar\n",
        "https://github.com/jcsobrino/TFM-Analisis_sentimientos_Twitter-UOC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtzJHYMJcQz3"
      },
      "source": [
        "###cargar librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esiw0R5hb9Ob"
      },
      "source": [
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E84yXpedZQ3"
      },
      "source": [
        "###permisos de acceso desde python a el api rest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_hwgEk2culC"
      },
      "source": [
        "consumer_key= 'akNwNm1GOHTYla7x0fZ8HbjKm'\n",
        "consumer_secret= 'mzI7oBKvmtd7M8LCqH4eOcicvIudZFWOqNr1gLxJfaDiNRnyrd'\n",
        "access_token= '154594065-SGLnI8Rtz2Q0u3XZAvj0Zm3TDE2ao5Cuttp3fr69'\n",
        "access_token_secret= 'uG9ut1xNvCs9y1acRFkcpYPp4NnBaDF2UfLMABDSIgNEN'\n",
        "\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rc9RsrNNbxI"
      },
      "source": [
        "###Subiendo un tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roIJgdUoden3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07d933d-ef08-4468-a490-f89dc03ac33d"
      },
      "source": [
        "api.update_status('#USTATUNJA,#DEEP_LEARNING Generación de Tweet con libreria tweepy desde PYTHON-> 20201104  16:15 p.m')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Status(_api=<tweepy.api.API object at 0x7fd5be26e6d8>, _json={'created_at': 'Wed Nov 04 21:15:12 +0000 2020', 'id': 1324097890302808065, 'id_str': '1324097890302808065', 'text': '#USTATUNJA,#DEEP_LEARNING Generación de Tweet con libreria tweepy desde PYTHON-&gt; 20201104  16:15 p.m', 'truncated': False, 'entities': {'hashtags': [{'text': 'USTATUNJA', 'indices': [0, 10]}, {'text': 'DEEP_LEARNING', 'indices': [11, 25]}], 'symbols': [], 'user_mentions': [], 'urls': []}, 'source': '<a href=\"https://www.ustatunja.edu.co/inicio-ingenieria-sistemas\" rel=\"nofollow\">usta_deep_learning</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 154594065, 'id_str': '154594065', 'name': 'LUIS FERNANDO CASTEL', 'screen_name': 'lcastellanos10', 'location': 'colombia, bucaramanga', 'description': 'Amante de los Deportes extremos, al buen cine y los bestseller,futbolista machaco, tenista regular y golfista frustrado..y próximamente competidor de Nairo jaja', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 122, 'friends_count': 81, 'listed_count': 0, 'created_at': 'Fri Jun 11 17:34:17 +0000 2010', 'favourites_count': 39, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 356, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '022330', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme9/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme9/bg.gif', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/154594065/1382366334', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': '181A1E', 'profile_sidebar_fill_color': '252429', 'profile_text_color': '666666', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'es'}, created_at=datetime.datetime(2020, 11, 4, 21, 15, 12), id=1324097890302808065, id_str='1324097890302808065', text='#USTATUNJA,#DEEP_LEARNING Generación de Tweet con libreria tweepy desde PYTHON-&gt; 20201104  16:15 p.m', truncated=False, entities={'hashtags': [{'text': 'USTATUNJA', 'indices': [0, 10]}, {'text': 'DEEP_LEARNING', 'indices': [11, 25]}], 'symbols': [], 'user_mentions': [], 'urls': []}, source='usta_deep_learning', source_url='https://www.ustatunja.edu.co/inicio-ingenieria-sistemas', in_reply_to_status_id=None, in_reply_to_status_id_str=None, in_reply_to_user_id=None, in_reply_to_user_id_str=None, in_reply_to_screen_name=None, author=User(_api=<tweepy.api.API object at 0x7fd5be26e6d8>, _json={'id': 154594065, 'id_str': '154594065', 'name': 'LUIS FERNANDO CASTEL', 'screen_name': 'lcastellanos10', 'location': 'colombia, bucaramanga', 'description': 'Amante de los Deportes extremos, al buen cine y los bestseller,futbolista machaco, tenista regular y golfista frustrado..y próximamente competidor de Nairo jaja', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 122, 'friends_count': 81, 'listed_count': 0, 'created_at': 'Fri Jun 11 17:34:17 +0000 2010', 'favourites_count': 39, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 356, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '022330', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme9/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme9/bg.gif', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/154594065/1382366334', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': '181A1E', 'profile_sidebar_fill_color': '252429', 'profile_text_color': '666666', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, id=154594065, id_str='154594065', name='LUIS FERNANDO CASTEL', screen_name='lcastellanos10', location='colombia, bucaramanga', description='Amante de los Deportes extremos, al buen cine y los bestseller,futbolista machaco, tenista regular y golfista frustrado..y próximamente competidor de Nairo jaja', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=122, friends_count=81, listed_count=0, created_at=datetime.datetime(2010, 6, 11, 17, 34, 17), favourites_count=39, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=356, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='022330', profile_background_image_url='http://abs.twimg.com/images/themes/theme9/bg.gif', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme9/bg.gif', profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', profile_banner_url='https://pbs.twimg.com/profile_banners/154594065/1382366334', profile_link_color='0084B4', profile_sidebar_border_color='181A1E', profile_sidebar_fill_color='252429', profile_text_color='666666', profile_use_background_image=True, has_extended_profile=True, default_profile=False, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none'), user=User(_api=<tweepy.api.API object at 0x7fd5be26e6d8>, _json={'id': 154594065, 'id_str': '154594065', 'name': 'LUIS FERNANDO CASTEL', 'screen_name': 'lcastellanos10', 'location': 'colombia, bucaramanga', 'description': 'Amante de los Deportes extremos, al buen cine y los bestseller,futbolista machaco, tenista regular y golfista frustrado..y próximamente competidor de Nairo jaja', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 122, 'friends_count': 81, 'listed_count': 0, 'created_at': 'Fri Jun 11 17:34:17 +0000 2010', 'favourites_count': 39, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 356, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '022330', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme9/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme9/bg.gif', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/154594065/1382366334', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': '181A1E', 'profile_sidebar_fill_color': '252429', 'profile_text_color': '666666', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, id=154594065, id_str='154594065', name='LUIS FERNANDO CASTEL', screen_name='lcastellanos10', location='colombia, bucaramanga', description='Amante de los Deportes extremos, al buen cine y los bestseller,futbolista machaco, tenista regular y golfista frustrado..y próximamente competidor de Nairo jaja', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=122, friends_count=81, listed_count=0, created_at=datetime.datetime(2010, 6, 11, 17, 34, 17), favourites_count=39, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=356, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='022330', profile_background_image_url='http://abs.twimg.com/images/themes/theme9/bg.gif', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme9/bg.gif', profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1244469051197440010/hxloNIad_normal.jpg', profile_banner_url='https://pbs.twimg.com/profile_banners/154594065/1382366334', profile_link_color='0084B4', profile_sidebar_border_color='181A1E', profile_sidebar_fill_color='252429', profile_text_color='666666', profile_use_background_image=True, has_extended_profile=True, default_profile=False, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none'), geo=None, coordinates=None, place=None, contributors=None, is_quote_status=False, retweet_count=0, favorite_count=0, favorited=False, retweeted=False, lang='es')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ4mzeX-NfBu"
      },
      "source": [
        "###Consultando tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzjhHn6vNrMG"
      },
      "source": [
        "# Definir el termino de la busqueda y la fecha de inicio\n",
        "search_words ='#ParoNacional29A'\n",
        "date_since ='2021-04-24'\n",
        "#para que no tome los tweets que estar retweets\n",
        "new_search = search_words+\" -filter:retweets\"\n",
        "new_search\n",
        "\u000b# Collecional tweets\n",
        "tweets = tw.Cursor(api.search,new_search,\"es\",date_since).items(100)\n",
        "#[tweet.text for tweet in tweets] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnsyptWENvg3"
      },
      "source": [
        "###Convertimos los tweets en una Dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reYRHI2zN0oG"
      },
      "source": [
        "data_frame = [[tweet.user.screen_name, tweet.user.location,tweet.text] for tweet in tweets]\n",
        "\n",
        "tw_dataframe = pd.DataFrame(data= data_frame , columns=[\"user\",\"location\",\"text\"])\n",
        "tw_dataframe\n",
        "#guardamos el dataframe en un CSV\n",
        "tw_dataframe.to_csv('/content/drive/MyDrive/Colab Notebooks/DataTwitter/ParoNacional29A.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgKDHgt8IRkz"
      },
      "source": [
        "tw_dataframe.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxiaP7qb2HIk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktPgWvbqOEBm"
      },
      "source": [
        "##P0. Obtener corpus : Convirtiendo XML a CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_N996vYOx2c"
      },
      "source": [
        "#librerias necesarias\n",
        "import xml.etree.ElementTree as etree\n",
        "import csv\n",
        "from os import scandir\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhCauzL5jkpN"
      },
      "source": [
        "###Funcion para listar archivos de un directorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQfljH0ukvuM"
      },
      "source": [
        "# importing os module  \n",
        "import os \n",
        "#listado_de_archivos_desde_un_path\n",
        "def files_of_path(path): \n",
        "    return [obj.name for obj in os.scandir(path) if obj.is_file()]\n",
        "    \n",
        "files= files_of_path(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017\")\n",
        "for file in files:\n",
        "    print(file)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrG7haSNtb6T"
      },
      "source": [
        "###función para convertir listas en archivos CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGxsb68nt1Co"
      },
      "source": [
        "def list_to_csv(data, filename):\n",
        "  with open(filename, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',', lineterminator='\\n', quoting=csv.QUOTE_NONNUMERIC)\n",
        "    writer.writerows(data)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvec1PiF5Ipz"
      },
      "source": [
        "###función para cargar de un CSV a  una LISTA (messages | labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBAnptZ04-We"
      },
      "source": [
        "def csv_to_lists(filename):\n",
        "  messages = []\n",
        "  labels = []\n",
        "  with open(filename, 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "      messages.append(row[1])\n",
        "      labels.append(row[2])\n",
        "  return messages, labels"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6wQAhv0V4RU"
      },
      "source": [
        "###funciones para prasear xml en un dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm6bJ8rDnJ8d"
      },
      "source": [
        "####corpus de general  tweetid | content | sentiments/polarity/value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSZHjRsUV9wj"
      },
      "source": [
        "def general_tass_to_list(filename):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = tweet.find('sentiments/polarity/value').text\n",
        "    data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkQa78F3K6Vp"
      },
      "source": [
        "def general_tass_2017_to_list(filename,qrel=None):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = qrel[tweetId]\n",
        "    data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "\n",
        "  return data"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYuKjPlHnvdV"
      },
      "source": [
        "####Corpus politics  tweetid | content | sentiments/polarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iybrjrHmnvzs"
      },
      "source": [
        "def politics_tass_to_list(filename):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    aux = next((e for e in tweet.findall('sentiments/polarity') if e.find('entity') == None), None)\n",
        "    if aux != None:\n",
        "      polarityValue = aux.find('value').text\n",
        "      data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6a_3I3ZoTc6"
      },
      "source": [
        "####corpus de internacional  tweetid | content | sentiments/polarity/value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGKq5CTgoT2h"
      },
      "source": [
        "def intertass_tass_to_list(filename, qrel=None):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = tweet.find('sentiment/polarity/value').text\n",
        "    if polarityValue == None:\n",
        "      polarityValue = qrel[tweetId]\n",
        "      data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0t1IZY6EzcB"
      },
      "source": [
        "####Parcear corpus de social-TV (no esta terminado)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXVrTziIUB8x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9e1f823-286e-407d-f1f9-a1604ad1d1eb"
      },
      "source": [
        "from lxml import etree\n",
        "doc = etree.parse('/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/Social-TV/socialtv-tweets-test.xml')\n",
        "#print (etree.tostring(doc,pretty_print=True ,xml_declaration=True, encoding=\"utf-8\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHREvJ4MUqQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "760377aa-f7b1-4bd4-ddc9-c60288672949"
      },
      "source": [
        "raiz=doc.getroot()\n",
        "#print (raiz.tag)\n",
        "#print (len(raiz))\n",
        "tweet=raiz[1]\n",
        "print (tweet.text)\n",
        "print (tweet.attrib)\n",
        "print (tweet[0].text)\n",
        "for attr,value in tweet.items():\n",
        "  print (attr,value)\n",
        "print (tweet.get(\"sentiment\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El \n",
            "{'id': '456544890499760129'}\n",
            "Barça\n",
            "id 456544890499760129\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6cEJOPYP92J"
      },
      "source": [
        "from xml.dom import minidom\n",
        "def social_tv_to_list(filename):\n",
        "  xmldoc = minidom.parse(filename)\n",
        "  tweetlist = xmldoc.getElementsByTagName('tweet')\n",
        "  print(len(tweetlist))\n",
        "  print(tweetlist[0].attributes['id'].value)\n",
        "  sentimentlist = xmldoc.getElementsByTagName('sentiment')\n",
        "  print(sentimentlist[0].attributes['aspect'].value)\n",
        "  #for s in tweetlist:\n",
        "#    print(s.attributes['id'].value)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulBbULqvQg10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1b8ddb7a-bdd6-4901-90ca-b62fb6bfc729"
      },
      "source": [
        "social_tv_to_list(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/Social-TV/socialtv-tweets-test.xml\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "456544890097131521\n",
            "Entrenador\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqoOWZwOsD6w"
      },
      "source": [
        "####funcion para unir los tweets corpus general test con sus sentimientos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YY91YATsEOq"
      },
      "source": [
        "#Listar los id tweets | sentiment :P (Positivo) - N (Negativo) - NEU (NEUtro) - NONE (sin sentimiento)\n",
        "def gold_standard_to_dict(filename):\n",
        "  with open(filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter='\\t')\n",
        "    data = {rows[0]: rows[1] for rows in reader}\n",
        "\n",
        "  return data"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W712Ry66vFjl"
      },
      "source": [
        "###Función para separar el 100% del corpus entre: Train : 70% - Test: 30%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDxtXKbvN4V"
      },
      "source": [
        "def generate_train_test_subsets(data, size):\n",
        "  codes = [d[0] for d in data]\n",
        "  labels = [d[2] for d in data]\n",
        "  codes_train, codes_test, labels_train, labels_test = train_test_split(codes, labels, train_size=size)\n",
        "  train_data = [d for d in data if d[0] in codes_train]\n",
        "  test_data = [d for d in data if d[0] in codes_test]\n",
        "  return train_data, test_data"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwBHQQFsqR7i"
      },
      "source": [
        "### **Ejecutar cada función de parsear los corpus y guardarlo en un CSV (full, train, test)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QImbKpQOgVu1"
      },
      "source": [
        "!git clone 'https://github.com/luisFernandoCastellanosG/Machine_learning'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FmpPZyXqcyX"
      },
      "source": [
        "data = []\n",
        "\n",
        "#Parceamos el internacional TASS\n",
        "#tomamos el corpus internacional (test) y generamos una lista del ID del tweet y el sentimiento para agregarlo a la data\n",
        "qrel = gold_standard_to_dict(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/InterTASS/InterTASS_Test_res.qrel\")\n",
        "#como el test del corpus internacional esta sin los sentimientos es necesario agregarlos : qrel\n",
        "data.extend(intertass_tass_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/InterTASS/InterTASS_Test.xml\", qrel))\n",
        "data.extend(intertass_tass_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/InterTASS/InterTASS_development.xml\"))\n",
        "data.extend(intertass_tass_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/InterTASS/InterTASS_Training.xml\"))\n",
        "#Parceamos el General\n",
        "#NO-data.extend(DatasetHelper.general_tass_to_list(\"../datasets/tass_2017/InterTASS/general-test-tagged-3l.xml\"))\n",
        "#NO-data.extend(DatasetHelper.general_tass_to_list(\"../datasets/tass_2017/InterTASS/general-train-tagged-3l.xml\"))\n",
        "qrel = gold_standard_to_dict(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/General Corpus of TASS/general-sentiment-3l.qrel\")\n",
        "data.extend(general_tass_2017_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/General Corpus of TASS/general-tweets-test.xml\", qrel))\n",
        "#Parceamos el STOMPOL (politica)\n",
        "data.extend(politics_tass_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2014/politics-test-tagged.xml\"))\n",
        "#separamos la data en train = 70%  | test = 30#\n",
        "test, train  = generate_train_test_subsets(data, size=0.3)\n",
        "list_to_csv(data,\"/content/dataset_2017_full.csv\")\n",
        "list_to_csv(train, '/content/dataset_2017_train.csv')\n",
        "list_to_csv(test, '/content/dataset_2017_test.csv')\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRF-yQ5j4qR6"
      },
      "source": [
        "##P1. Preprocesamiento del corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlERt98W5qLZ"
      },
      "source": [
        "###Cargar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ0W5EYT5s2B"
      },
      "source": [
        "import re                                #operaciones regulares para la búsqueda y manipulación de cadenas\n",
        "from nltk import TweetTokenizer          #libreria para tokenizar\n",
        "from nltk.stem import SnowballStemmer    #algoritmo para clasificación de palabras\n",
        "#variables para mejorar la escritura (opcional)\n",
        "NORMALIZE = 'normalize'\n",
        "REMOVE = 'remove'\n",
        "MENTION = 'twmention'\n",
        "HASHTAG = 'twhashtag'\n",
        "URL = 'twurl'\n",
        "LAUGH = 'twlaugh'\n",
        "\n",
        "#definir que el algoritmo de clasificación use el idioma español\n",
        "_stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "#definir una variable para la funcion de tokenizar (opcional)\n",
        "_tokenizer = TweetTokenizer().tokenize\n",
        "\n",
        "#variable para definir si quiero normalizar: normalize o eliminar: remove los hashtags, menciones y urls en los tweets\n",
        "_twitter_features=\"normalize\"\n",
        "#variable para definir si se desea tener convertir o no a la raiz de la palabra.\n",
        "_stemming=False"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrJ7vHEWNAbd"
      },
      "source": [
        "### funciones/métodos de preprocesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTB4zzjoL1Rs"
      },
      "source": [
        "####listas de conversión (quitar tildes y palabras coloquiales) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tj86lDGL6cU"
      },
      "source": [
        "#lista de conversión para quitar las tildes a las vocales.\n",
        "DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]\n",
        "\n",
        "#lista para corregir algunas palabras coloquiales / jerga en español (obviamente faltan más)\n",
        "SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
        "         ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),\n",
        "         ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas')]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCCRddegNIDA"
      },
      "source": [
        "#### funcion/método de normalización de risas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lvGhh5vNMEc"
      },
      "source": [
        "#metodo para normalizar las risas\n",
        "def normalize_laughs(message):\n",
        "  message = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[k])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[h])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(juas+|lol)\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOW2UQjqN6Eh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc55a1a-800e-47c8-baef-c4fc05ab2d32"
      },
      "source": [
        "print (normalize_laughs(\"esto muyy feliz jajajajaja o no tan feliz jejejejeje o mejor me rio a como papa noel JOJOJO o como en mileniams LOL  kakaka\"))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "esto muyy feliz twlaugh o no tan feliz twlaugh o mejor me rio a como papa noel twlaugh o como en mileniams twlaugh  twlaugh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3qBGgvpRgZL"
      },
      "source": [
        "####Función/método para eliminar o normalizar  menciones, hashtags y URL de un mensaje (tweet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpdPu3K8RukN"
      },
      "source": [
        "def process_twitter_features(message, twitter_features):\n",
        "\n",
        "  message = re.sub(r'[\\.\\,]http','. http', message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'[\\.\\,]#', '. #', message)\n",
        "  message = re.sub(r'[\\.\\,]@', '. @', message)\n",
        "\n",
        "  if twitter_features == REMOVE:\n",
        "    # eliminar menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))(@|#)\\S+', '', message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', '', message, flags=re.IGNORECASE)\n",
        "  elif twitter_features == NORMALIZE:\n",
        "    # cuando sea necesario se normalizaran las menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))@\\S+', MENTION, message)\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))#\\S+', HASHTAG, message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', URL, message, flags=re.IGNORECASE)\n",
        "\n",
        "  return message"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS3BzP5OR_hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9c31fb-b93a-4ffb-91f3-b07a0f63cf1a"
      },
      "source": [
        "print(process_twitter_features(\"Rosell, una noche. Adivina quien!! http://t.co/PPAwijRX, jajajAja dime si NO ES DÍVERTIDÓÓ\",\"normalize\"))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rosell, una noche. Adivina quien!! twurl, jajajAja dime si NO ES DÍVERTIDÓÓ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCS2HmN4V_hi"
      },
      "source": [
        "####Función/método general para el preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3T1DgbnWFYG"
      },
      "source": [
        "def preprocess(message):\n",
        "  # convertir a minusculas\n",
        "  message = message.lower()\n",
        "        \n",
        "  # eliminar números, retorno de linea y el tan odios retweet (de los viejos estilos de twitter)\n",
        "  message = re.sub(r'(\\d+|\\n|\\brt\\b)', '', message)\n",
        "        \n",
        "  # elimar vocales con signos diacríticos (posible ambigüedad)\n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    message = re.sub(r'{0}'.format(s), t, message)\n",
        "        \n",
        "  # eliminar caracteres repetidos \n",
        "  message = re.sub(r'(.)\\1{2,}', r'\\1\\1', message)\n",
        "       \n",
        "  # normalizar las risas\n",
        "  message = normalize_laughs(message)\n",
        "        \n",
        "  # traducir la jerga y terminos coloquiales sobre todo en el español\n",
        "  for s,t in SLANG:\n",
        "    message = re.sub(r'\\b{0}\\b'.format(s), t, message)\n",
        "\n",
        "  #normalizar/eliminar hashtags, menciones y URL\n",
        "  message = process_twitter_features(message, _twitter_features)\n",
        "\n",
        "  #Convertir las palabras a su raiz ( Bonita, bonito) -> bonit \n",
        "  if _stemming:\n",
        "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
        "\n",
        "  return message"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCkSi7RJW6FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8affb2f2-e65b-4334-e2e6-583ea9f6c79d"
      },
      "source": [
        "print(preprocess(\"LOL!! muy graciosa esta paguina https://actualidadpanamericana.com :-) jajajaja muy buena\"))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "twlaugh!! muy graciosa esta paguina twurl :-) twlaugh muy buena\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aWOdOb167P7"
      },
      "source": [
        "###Descargamos la librerias  NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSp_L2h2_I36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b51e69-80d8-4ee8-c53a-d31579fd190c"
      },
      "source": [
        "#Descargamos la libreria de stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIJQSCntSPOl"
      },
      "source": [
        "### Cargamos el CSV del corpus de google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVtJMBDy_slb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e16d27-9f76-4c55-8d8c-cc4e65541441"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOE6fCqXcUXn"
      },
      "source": [
        "###Aplicamos preprocesamiento al CSV y creamos un nuevo CSV limpio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ods0bwtGCWo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/luisFernandoCastellanosG/Machine_learning/master/Analisis_sentimientos_Twitter/espanish/datasets/Corpus/dataset_2017_full.csv', encoding='utf-8')\n",
        "#asignamos nombres a las columnas del csv para facilitar la busqueda de información\n",
        "df.columns = ['tweetid', 'tweet','sentiment']\n",
        "#aplicamos el preprocesamiento a los tweets con steaming =false\n",
        "df['tweet'] = df['tweet'].apply(preprocess)\n",
        "#eliminamos la columna tweetid que no nos sirve para entrenar y si nos genera mas uso de memoria \n",
        "df = df.drop(columns=\"tweetid\")\n",
        "#Es mejor trabajar con valores enteros que con letras\n",
        "#por lo tanto reemplazaremos los sentimientos que estan como NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "df.loc[df['sentiment'] == 'NONE', 'sentiment'] = '-1'\n",
        "df.loc[df['sentiment'] == 'NEU', 'sentiment'] = '0'\n",
        "df.loc[df['sentiment'] == 'P', 'sentiment'] = '1'\n",
        "df.loc[df['sentiment'] == 'N', 'sentiment'] = '2'\n",
        "df[\"sentiment\"].unique()\n",
        "#guardamos el dataset en un nuvevo CSV para facilitar su posterior uso\n",
        "df.to_csv('/content/dataset_2017_full_clean.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAn4p8Z_Y8sH"
      },
      "source": [
        "##P2.Entrenando el modelo de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3kl_mdMN4ec"
      },
      "source": [
        "###Funciones de tokenizar/extraer tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5UTVDCZZBYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee46126-2fd3-4cb4-9326-209c0651f43b"
      },
      "source": [
        "#p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
        "print(\"p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\")\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "#p2.2: funcion para extraer un documento del dataset  \n",
        "print(\"p2.2: funcion para extraer un documento del dataset  \")\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv)  # skip header\n",
        "        for line in csv:\n",
        "            text, label = line[:-3],  int(line[-2])\n",
        "            yield text, label\n",
        "#p2.3: funcion que tomara una secuencia de documentos y devolvera un número particular de documentos\n",
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
            "p2.2: funcion para extraer un documento del dataset  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuILfqd2hq1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dc7bf3-7d6e-4e33-c751-ed9e9f2def8a"
      },
      "source": [
        "next(stream_docs(path='/content/dataset_2017_full_clean.csv'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('twmention ya era hora de volver al csgo y dejares el padel bienvenida ', 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fcBk57OXCz3"
      },
      "source": [
        "###Entrenamos el modelo usando regresión logistica\n",
        "usaremos regresión logistica xq es menos costoso en tiempo de procesamiento que support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCwTY8TCM3HE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e22b1870-d78f-43aa-da32-499c5d472758"
      },
      "source": [
        "path='/content/dataset_2017_full_clean.csv'\n",
        "#p2: definimos una versión liviana de CountVectorizer+TfidfVectorizer llamada HashingVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore', \n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None, \n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "#definimos como algoritmo la regressión logistica en el decenso gradiante \n",
        "\n",
        "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
        "doc_stream = stream_docs(path)\n",
        "#p3. entrenamos \n",
        "import re\n",
        "import numpy as np\n",
        "#import pyprind\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('spanish')\n",
        "#pbar = pyprind.ProgBar(50)\n",
        "#definimos las clases con las cuales vamos a entrenar\n",
        "classes = np.array([-1,0, 1,2])\n",
        "#hacemos 50 repeticiones\n",
        "for _ in range(50):\n",
        "  #tomaremos grupos de 500 tweets para entrenar\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "    #pbar.update()\n",
        "#probamos la eficiencia del modelo con 5000 tweets .\n",
        "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
        "X_test = vect.transform(X_test)\n",
        "print('Presición del modelo: %.3f' % clf.score(X_test, y_test))\n",
        "#recalibramos el modelo.\n",
        "clf = clf.partial_fit(X_test, y_test)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Presición del modelo: 0.806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLOtd2p7X0yn"
      },
      "source": [
        "##P3.Serializamos (congelamos) el modelo para usarlo fuera de google colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRxlktL3X-Q_"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "#creo una carpeta en mi google drive para guardar los archivos serializados\n",
        "dest = os.path.join('/content/twitterclassifier', 'pkl_objects')\n",
        "if not os.path.exists(dest):\n",
        "    os.makedirs(dest)\n",
        "#convertimos el clasificador y el stopword en archivo/objectos pkl\n",
        "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
        "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)\n",
        "#Es importante recordar que deben verificar que los dos archivos esten en su drive"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atFbFUcmZuwZ"
      },
      "source": [
        "###Probemos a ver si funciona\n",
        "Cambiamos la basepath (directorio por defecto) de Python a la carpeta de **Twitterclassifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moUaKeQOZye5"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/twitterclassifier') "
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWSBw2L6aJri"
      },
      "source": [
        "####Deserializamos los estimadores "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCQ2qYrsZ_67"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "import os\n",
        "from vectorizer import vect  # archivo vectorizer.py \n",
        "clf = pickle.load(open(os.path.join('/content/twitterclassifier/pkl_objects', 'classifier.pkl'), 'rb'))"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR3R_vU3aY_L"
      },
      "source": [
        "#### Clasifiquemos un texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVpOVv8tacSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079a8885-45c0-4c06-f4b3-0fcf5c071cf7"
      },
      "source": [
        "import numpy as np\n",
        "#NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}\n",
        "\n",
        "#example = ['Te odio más que a la muerte']\n",
        "example1 = 'amo este planeta, quiero ser feliz'\n",
        "example = [example1]\n",
        "#convertimos el texto en un vector de palabras y extraemos sus caracteristicas https://scikit-learn.org/stable/modules/feature_extraction.html\n",
        "textConvert = vect.transform(example)  \n",
        "print('*Predicción: %s\\n*Probabilidad: %.2f%%'%(label[clf.predict(textConvert)[0]], np.max(clf.predict_proba(textConvert))*100))\n",
        "print('*Predicción: %s'%label[clf.predict(textConvert)[0]])\n",
        "print(np.max(clf.predict_proba(textConvert))*100)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*Predicción: Positivo\n",
            "*Probabilidad: 87.70%\n",
            "*Predicción: Positivo\n",
            "87.70442129985135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiZauNoCbrxG"
      },
      "source": [
        "###**RECORREMOS LOS TWEETS DESCARGADOS Y LOS CLASIFICAMOS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL1EUqECSjP3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyprind\n",
        "\n",
        "pbar = pyprind.ProgBar(50000)\n",
        "\n",
        "df = pd.read_csv('/content/EleccionesUSA2020_data.csv', encoding='utf-8')\n",
        "#creamos una columna llamada Sentimient donde guardaremos la predicción\n",
        "df['sentiment'] =''\n",
        "#creamos una columna llamada Probability donde guardaremos la acertabilidad que dio el clasificador\n",
        "df['probability']=0\n",
        "#conversión de sentimientos (numeros a palabras)= NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}\n",
        "for rowid in range(len(df.index)):\n",
        "  text=df['text'][rowid]\n",
        "  textConvert = vect.transform([text]) \n",
        "  df['sentiment'][rowid]=label[clf.predict(textConvert)[0]]\n",
        "  df['probability'][rowid]=np.max(clf.predict_proba(textConvert))*100\n",
        "  pbar.update()\n",
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZesOYw9FVF8e"
      },
      "source": [
        "\n",
        "df.to_csv('/content/EleccionesUSA2020_data_sentiment.csv'', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Noxdqzav-LF"
      },
      "source": [
        "#segunda forma de ejecutar el analisis (metodos)\n",
        "def f_prediction(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return label[clf.predict(textConvert)[0]]\n",
        "\n",
        "def f_probability(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return np.max(clf.predict_proba(textConvert))*100\n",
        "\n",
        "df[\"sentiment\"] = df.apply(f_prediction, axis=1) # recorriendo columnas\n",
        "df[\"probability\"] = df.apply(f_probability, axis=1) # recorriendo columnas\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lh2gY17uFeG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "182b420d-952e-4c34-e879-b37e117c9335"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#sentimientos = df[\"sentiment\"].unique()\n",
        "df.groupby('sentiment')['location'].nunique().plot(kind='bar')\n",
        "print(df.groupby(['sentiment']).size())\n",
        "#df.groupby(['sentiment']).size().unstack().plot(kind='bar',stacked=True)\n",
        "plt.show()\n",
        "\n",
        "#df.head(20)\n",
        "#df[\"sentiment\"].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentiment\n",
            "Negativo     64\n",
            "Positivo    936\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0KL67R0OhrU"
      },
      "source": [
        "##p4. generar DB de sqlite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdCkpWREOl5Q"
      },
      "source": [
        "import sqlite3\n",
        "import os\n",
        "\n",
        "if os.path.exists('tweets.sqlite'):\n",
        "    os.remove('tweets.sqlite')\n",
        "\n",
        "conn = sqlite3.connect('tweets.sqlite')\n",
        "c = conn.cursor()\n",
        "c.execute('CREATE TABLE tweets_db (tweet TEXT, sentiment INTEGER, date TEXT)')\n",
        "\n",
        "example1 = 'que aburrido es estar en cuarentena…'\n",
        "c.execute(\"INSERT INTO tweets_db (tweet, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example1, 2))\n",
        "\n",
        "example2 = 'Estoy feliz de estar con mi familia'\n",
        "c.execute(\"INSERT INTO tweets_db (tweet, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example2, 1))\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGveeTAiO5SQ"
      },
      "source": [
        "###P4.1 consultar db"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qBTUzNEO8Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca19dd32-811a-490b-8354-3d97a9378a39"
      },
      "source": [
        "conn = sqlite3.connect('tweets.sqlite')\n",
        "c = conn.cursor()\n",
        "\n",
        "c.execute(\"SELECT * FROM tweets_db WHERE date BETWEEN '2019-01-01 10:10:10' AND DATETIME('now')\")\n",
        "results = c.fetchall()\n",
        "conn.close()\n",
        "\n",
        "print(results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('que aburrido es estar en cuarentena…', 2, '2020-11-11 21:33:58'), ('Estoy feliz de estar con mi familia', 1, '2020-11-11 21:33:58')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}